---
title: "Likelihood and Model Fit: A Visual Tour"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
require(learnr)
require(ggplot2)
require(plotly)
require(lhs)
knitr::opts_chunk$set(echo = FALSE)
```



## A simple linear model

Our first task will be to generate some data from a linear model of the classic sort, i.e. $y_i \sim Normal(\mu_i, \sigma)$.

```{r, echo=FALSE}
sidebarPanel(
sliderInput("beta", "Slope:", min = 0.1, max = 5, value = 2),
sliderInput("alpha", "Intercept:", min = 0.1, max = 5, value = 0.1),

sliderInput("sd", "Error Standard Deviation:", min = 0.1, max = 4, value = 1)
)

mainPanel(
plotlyOutput("distPlot")
)
```


```{r, context="server"}
xvals <- seq(from = 0.0, to = 10, by  = 0.05)
y <- reactive({rnorm(length(xvals), input$alpha + input$beta*xvals, input$sd)})
real_ll <- reactive({dnorm(y(), input$alpha + input$beta*xvals, input$sd, log=TRUE)})

output$distPlot <- renderPlotly({
  g <- ggplot(data.frame(x=xvals, y=y()), aes(x=x)) + 
    geom_point(aes(y=y)) + 
    xlim(0, 10) + 
    ylim(0, 60) 
  return(ggplotly(g))
})

```

### Try to find the Maximum Likelihood Estimate

The line in the figure below is a regression line with intercept and slope set by the sliders. 

Move the sliders around and see if you can find where the model maximizes the *likelihood* of the data (darker colors = greater likelihood):

```{r, echo = FALSE}

sidebarPanel(
sliderInput("beta_pred", "Slope:", min = -5, max = 5, value = 0),
sliderInput("alpha_pred", "Intercept:", min = -50, max = 50, value = 0),

sliderInput("sd_pred", "Error Standard Deviation:", min = 0.1, max = 4, value = 1)
)

mainPanel(
plotlyOutput("llPlot")
)

```




```{r, context="server"}
pred_y <-  reactive({input$alpha_pred + input$beta_pred*xvals})

data_logll <- reactive({
  z <- dnorm(y(), pred_y(), input$sd_pred, log = TRUE) 
  return(z-real_ll())
          })


output$llPlot <- renderPlotly({
  g <- ggplot(data.frame(x=xvals, y=y(), yp = pred_y(), z=data_logll()), aes(x=x)) + 
    geom_point(aes(y=y,color=z)) + 
    geom_line(aes(y=yp)) +
    xlim(0, 10) + 
    ylim(-60, 60) +
    guides(color=guide_legend(title="Difference from MLE")) + 
    scale_color_distiller(name = "Difference from MLE", type="div", palette="spectral")

  ggplotly(g)
})

```

What impact does the variance seem to have relative to the intercept and slope?


## Adding an *exposure*

Up to this point, we have focused on the relationship between a single variable and an outcome. Now let's make things a bit more interesting and look at what happens when our data have very simple *clustering*.

Here we are assuming that *increased* household wealth is associated with decreased systolic blood pressure (SBP) as in the Merlo example. In our version, though, we're imagining there are just two neighborhoods: A 'normal' one where the relationship between wealth and SBP holds, and an 'outlier' one that is associated with an *increase* in SBP for anyone who lives there, regardless of wealth.

The points in the figure represent SBP observations for individuals in each neighborhood, and the regression line shows the predictions of a mdoel that ignores this clustering and just estimates the intercept and wealth effects.


```{r, echo=FALSE}
sidebarLayout(
sidebarPanel(
sliderInput("beta_1", "Wealth Effect:", min = -10, max = 0, value = 0),
sliderInput("beta_2", "Neighborhood Effect:", min = 0, max = 50, value = 10),
sliderInput("alpha_bp", "Intercept:", min = 120, max = 200, value = 150),
),

mainPanel(
  plotOutput("clusterPlot"),
  
)
)
```


```{r, context="server"}
xvals <- seq(from = 0.0, to = 10, by  = 0.05)

y_cluster <- reactive({
  
  ## Sample proportion in outlier area
  p_outlier <- plogis(qlogis(input$p_n) + log(input$o_n)*xvals)
  outlier_n <- rbinom(length(xvals), 1, p_outlier)
  
  ## Sample outcome values
  y_hat <- input$alpha_bp + input$beta_1*xvals + input$beta_2*outlier_n
  vals <- rnorm(length(xvals), y_hat, input$sd)
  true_ll <- dnorm(vals, y_hat, input$sd, log=TRUE)
  
  return(data.frame(x = xvals,
                    y = vals,
                    outlier = outlier_n,
                    logll= true_ll))
        })

  model_fit <- reactive({
    if (input$adjust == TRUE) {
      fit <- lm(y ~ x + outlier, data = y_cluster())
      names(fit$coefficients) <- c("Intercept", "Wealth", "Outlier")
      
    } else {

     fit <- lm(y ~ x, data = y_cluster())
       names(fit$coefficients) <- c("Intercept", "Wealth")
    }
    return(fit)
  })
  
  output$model_summary <- renderPrint({
    summary(model_fit())
  })


output$clusterPlot <- renderPlot({
  
  point_ll <- dnorm(y_cluster()$y, predict(model_fit()), input$sd, log=TRUE) - y_cluster()$logll
  
  g <- ggplot(y_cluster(), aes(x=x)) + 
    geom_point(aes(y=y, colour = point_ll)) + 
    xlim(0, 10) + 
    ylim(0, 210)
  
  if (input$adjust == "TRUE") {
      g <- g + geom_smooth(method = "lm", aes(y=y, group=outlier))
  } else {
      g <- g + geom_smooth(method = "lm", aes(y=y))

  }
  plot(g)
})

```

Below, you can see the results of fitting a linear model to the data. Take a look at the estimated *intercept* and *slope* for the wealth effect. What happens to these estimates when you make residence in the outlier neighborhood *dependent* on wealth?


```{r}
sidebarPanel(
sliderInput("p_n", "Proportion of poorest in outlier neighborhood:", min = 0.01, max = 0.5, value = 0.5),

sliderInput("o_n", "Odds ratio of living in outlier for each unit increase in wealth:", min = 1.0, max = 20.0, value = 1.0),

checkboxInput("adjust", "Adjust for neighborhood:", FALSE)

)

mainPanel(
  verbatimTextOutput("model_summary")
)
```

## More clustering!

Now, let's go crazy with a more multilevel example. We'll repeat the example from the last page, but imagine individuals are grouped into discrete neighborhoods by wealth (not an unrealistic assumption), and that wealth remains protective against SBP.

Now, we'll let some factor that is potentially associated with SBP risk, e.g. walkability, vary across neighborhoods, also as a function of *increasing neighborhood-level wealth*. 

Play with the sliders below, and see what happens to the across- and within-neighborhood relationships between wealth and SBP as you modify the intensity of the neighborhood risk factor.

```{r}
sidebarPanel(
  
sliderInput("beta_1_n", "Wealth Effect:", min = -10, max = 0, value = -5),
sliderInput("beta_2_n", "Neighborhood Effect:", min = 0, max = 50, value = 0),
sliderInput("alpha_n", "Intercept:", min = 120, max = 200, value = 150),

)

mainPanel(
 plotOutput("neighborhoodPlot")
)


```

Now, let's see what this looks like in modeling terms when we do and don't adjust for neighborhood:

```{r}
sidebarPanel(

checkboxInput("n_adjust", "Adjust for neighborhood:", FALSE)

)

mainPanel(
  verbatimTextOutput("n_model_summary")
)

```

```{r, context = "server"}
group_indices <- function(N, J) {
    out_groups <- c()
    split_size <- N %/% J

    for (i in 1:(J - 1)) {
        out_groups <- c(out_groups, rep(i, split_size))
    }

    if ((N %% split_size) > 0) {
        last_indices <- rep(J, N %% split_size)
    } else {
        last_indices <- rep(J, split_size)
    }
    out_groups <- c(out_groups, last_indices)

    return(out_groups)
}



area_data <- reactive({
  
J <- 5
N <- 1000

xvals <- 10 * sort(as.vector(randomLHS(N, 1)))
group_ids <- group_indices(N, J)
walkability <- (group_ids - 1) / (J - 1)

y_hat <- input$alpha_n + input$beta_1_n * xvals + input$beta_2_n * walkability

y_sim <- rnorm(length(y_hat), y_hat, 2)

df <- data.frame(x = xvals, walkability = walkability, neighborhood = as.factor(group_ids), y = y_sim)

return(df)

})

output$neighborhoodPlot <- renderPlot({
g <- ggplot(area_data(), aes(x = x, y = y)) +
    geom_point(aes(colour = neighborhood)) +
    xlab("Wealth") +
    ylab("SBP")
    
    if (input$n_adjust == TRUE) {
      g <- g+ geom_smooth(method = "lm", aes(group = neighborhood), se = FALSE) 
    } else {
      g <- g + geom_smooth(method = "lm", se = FALSE) 
    }


return(g)
})

 n_model_fit <- reactive({
    if (input$n_adjust == TRUE) {
      fit <- lm(y ~ x + neighborhood , data = area_data())
             names(fit$coefficients) <- c("Intercept", "Wealth", "Neighborhood 2", "Neighborhood 3", "Neighborhood 4", "Neighborhood 5")
      
    } else {

     fit <- lm(y ~ x, data = area_data())
       names(fit$coefficients) <- c("Intercept", "Wealth")
    }
    return(fit)
  })
  
  output$n_model_summary <- renderPrint({
    summary(n_model_fit())
  })
```

Now, try to see if you can completely wash out the impact of wealth vs. neighborhood.

After that, see if you can get the wealth effect to *reverse*!